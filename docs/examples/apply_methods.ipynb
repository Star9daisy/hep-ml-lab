{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply methods\n",
    "In this notebook we will show how to load dataset saved in the previous notebook\n",
    "and apply built-in methods on it. Finally we summarize their performance into a\n",
    "table\n",
    "\n",
    "## Introduction to built-in methods\n",
    "Currently `hml` have three types of methods: cuts, trees, and (neural) networks.\n",
    "\n",
    "With `hml.methods.cuts.CutAndCount`, we apply a series of cuts on the input data\n",
    "to select as many signal events and as few background events as possible. Each\n",
    "cut reduces the number of background events and inevitably also signal events.\n",
    "The goal usually is to find a set of cuts that maximizes the significance.\n",
    "\n",
    "The boosted decision tree is one of trees methods. It is a machine learning\n",
    "method commonly used in high energy physics. The name \"boosted\" comes from the\n",
    "idea to combine weak classifiers into a strong one. \n",
    "\n",
    "The neural network now only contains a simple fully connected neural network\n",
    "named `ToyMLP`.\n",
    "\n",
    "## Load the dataset\n",
    "First we load the dataset from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hml.datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load dataset, we use the class method `load` of `Dataset` class. The method\n",
    "takes the dataset directory as input and returns a `Dataset` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Train and test shapes:\n",
      "x_train shape: (12917, 4) y_train shape: (12917, 2)\n",
      "x_test shape: (3230, 4) y_test shape: (3230, 2)\n",
      "> target names: ['pp2jj', 'pp2zz']\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "dataset = Dataset.load(\"./data/z_vs_qcd\")\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    dataset.data, dataset.target, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert the labels to categorical\n",
    "y_train = to_categorical(y_train, dtype=\"int32\")\n",
    "y_test = to_categorical(y_test, dtype=\"int32\")\n",
    "\n",
    "# Show the shape of the training and testing sets\n",
    "print(\"> Train and test shapes:\")\n",
    "print(\"x_train shape:\", x_train.shape, \"y_train shape:\", y_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape, \"y_test shape:\", y_test.shape)\n",
    "print(f\"> target names: {dataset.target_names}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply methods\n",
    "Then we apply a boosted decision tree. It comes from `scikit-learn` package\n",
    "originally. The `compile` method takes loss function name, optimizer name, and a\n",
    "list of metrics as input. Here we use default parameters as in `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hml.methods import CutAndCount, BoostedDecisionTree, ToyMLP\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "from keras.metrics import CategoricalAccuracy\n",
    "from hml.metrics import MaxSignificance, RejectionAtEfficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Training model:\n",
      "Iter 1/10 - loss: 1.2097 - acc: 0.8795 - max_sig: 209.1248 - r50: 793.0361\n",
      "Iter 2/10 - loss: 1.0733 - acc: 0.9162 - max_sig: 270.3814 - r50: 185.3986\n",
      "Iter 3/10 - loss: 0.9599 - acc: 0.9328 - max_sig: 327.6703 - r50: 669.1434\n",
      "Iter 4/10 - loss: 0.8636 - acc: 0.9407 - max_sig: 379.0039 - r50: 324.4439\n",
      "Iter 5/10 - loss: 0.7811 - acc: 0.9460 - max_sig: 424.5269 - r50: 540.7352\n",
      "Iter 6/10 - loss: 0.7090 - acc: 0.9502 - max_sig: 463.4917 - r50: 396.5432\n",
      "Iter 7/10 - loss: 0.6471 - acc: 0.9528 - max_sig: 500.4489 - r50: 393.4188\n",
      "Iter 8/10 - loss: 0.5926 - acc: 0.9551 - max_sig: 634.4835 - r50: 588.6602\n",
      "Iter 9/10 - loss: 0.5450 - acc: 0.9571 - max_sig: 789.3623 - r50: 462.1482\n",
      "Iter 10/10 - loss: 0.5021 - acc: 0.9586 - max_sig: 799.2162 - r50: 580.2921\n"
     ]
    }
   ],
   "source": [
    "method1 = BoostedDecisionTree(n_estimators=10)\n",
    "method1.compile(\n",
    "    metrics=[\n",
    "        CategoricalAccuracy(name=\"acc\"),\n",
    "        MaxSignificance(name=\"max_sig\"),\n",
    "        RejectionAtEfficiency(name=\"r50\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"> Training model:\")\n",
    "history = method1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Training model:\n",
      "Cut 1/4 - loss: 1.9366 - acc: 0.8798 - max_sig: 113.1778 - r50: 8.2616\n",
      "Cut 2/4 - loss: 2.1924 - acc: 0.8719 - max_sig: 173.7675 - r50: 15.8622\n",
      "Cut 3/4 - loss: 3.8445 - acc: 0.8351 - max_sig: 209.4424 - r50: 23.7669\n",
      "Cut 4/4 - loss: 4.3686 - acc: 0.8086 - max_sig: 237.2822 - r50: 31.6540\n"
     ]
    }
   ],
   "source": [
    "method2 = CutAndCount()\n",
    "method2.compile(\n",
    "    loss=CategoricalCrossentropy(),\n",
    "    metrics=[\n",
    "        CategoricalAccuracy(name=\"acc\"),\n",
    "        MaxSignificance(name=\"max_sig\"),\n",
    "        RejectionAtEfficiency(name=\"r50\"),\n",
    "    ],\n",
    ")\n",
    "print(\"> Training model:\")\n",
    "history = method2.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Training model:\n",
      "Epoch 1/10\n",
      "51/51 - 6s - loss: 0.9719 - acc: 0.8862 - max_sig: 186.6020 - r50: 31.5840 - 6s/epoch - 117ms/step\n",
      "Epoch 2/10\n",
      "51/51 - 1s - loss: 0.8845 - acc: 0.8881 - max_sig: 204.8537 - r50: 38.1710 - 1s/epoch - 23ms/step\n",
      "Epoch 3/10\n",
      "51/51 - 1s - loss: 0.7423 - acc: 0.8981 - max_sig: 209.3404 - r50: 44.6123 - 1s/epoch - 22ms/step\n",
      "Epoch 4/10\n",
      "51/51 - 1s - loss: 0.6906 - acc: 0.9013 - max_sig: 219.8567 - r50: 52.8738 - 1s/epoch - 24ms/step\n",
      "Epoch 5/10\n",
      "51/51 - 1s - loss: 0.6771 - acc: 0.9052 - max_sig: 225.4868 - r50: 54.4882 - 1s/epoch - 24ms/step\n",
      "Epoch 6/10\n",
      "51/51 - 1s - loss: 0.6907 - acc: 0.8941 - max_sig: 217.4594 - r50: 41.9881 - 1s/epoch - 24ms/step\n",
      "Epoch 7/10\n",
      "51/51 - 1s - loss: 0.5566 - acc: 0.9212 - max_sig: 222.2641 - r50: 76.7521 - 1s/epoch - 25ms/step\n",
      "Epoch 8/10\n",
      "51/51 - 1s - loss: 0.5627 - acc: 0.9083 - max_sig: 227.2444 - r50: 70.6728 - 1s/epoch - 22ms/step\n",
      "Epoch 9/10\n",
      "51/51 - 1s - loss: 0.5455 - acc: 0.9077 - max_sig: 235.5335 - r50: 43.2605 - 1s/epoch - 24ms/step\n",
      "Epoch 10/10\n",
      "51/51 - 1s - loss: 0.4913 - acc: 0.9125 - max_sig: 239.4212 - r50: 73.5870 - 1s/epoch - 23ms/step\n"
     ]
    }
   ],
   "source": [
    "method3 = ToyMLP(input_shape=(x_train.shape[1],))\n",
    "method3.compile(\n",
    "    loss=CategoricalCrossentropy(),\n",
    "    metrics=[\n",
    "        CategoricalAccuracy(name=\"acc\"),\n",
    "        MaxSignificance(name=\"max_sig\"),\n",
    "        RejectionAtEfficiency(name=\"r50\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"> Training model:\")\n",
    "history = method3.fit(x_train, y_train, epochs=10, batch_size=256, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the performance\n",
    "Finally we compare the performance of the three methods. We use the `evaluate`\n",
    "to evaluate the performance via the loss and metrics we specified in the\n",
    "`compile` method. The `evaluate` method returns a dictionary of the loss and\n",
    "metrics.\n",
    "\n",
    "Here we use the `tabulate` function from `tabulate` package to summarize the\n",
    "performance into a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.2611 - acc: 0.9586 - max_sig: 601.7032 - r50: 647.3771\n",
      "loss: 4.4163 - acc: 0.8037 - max_sig: 243.9667 - r50: 33.6241\n",
      "101/101 - 4s - loss: 0.5475 - acc: 0.9350 - max_sig: 111.5401 - r50: 444.2333 - 4s/epoch - 44ms/step\n",
      "> Results:\n",
      "name                     loss     acc    max_sig       r50\n",
      "---------------------  ------  ------  ---------  --------\n",
      "boosted_decision_tree  0.2611  0.9586   601.7032  647.3771\n",
      "cut_and_count          4.4163  0.8037   243.9667   33.6241\n",
      "toy_mlp                0.5475  0.9350   111.5401  444.2333\n"
     ]
    }
   ],
   "source": [
    "results1 = method1.evaluate(x_test, y_test)\n",
    "results2 = method2.evaluate(x_test, y_test)\n",
    "results3 = method3.evaluate(x_test, y_test, verbose=2)\n",
    "results = {}\n",
    "\n",
    "results['name'] = [method1.name, method2.name, method3.name]\n",
    "for k in results1.keys():\n",
    "    results[k] = results1[k] + results2[k] + results3[k]\n",
    "\n",
    "print(\"> Results:\")\n",
    "print(tabulate(results, headers=\"keys\", floatfmt=\".4f\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
